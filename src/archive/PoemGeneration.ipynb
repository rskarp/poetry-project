{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10faccb",
   "metadata": {},
   "source": [
    "# Install and Import Needed Packages and Models\n",
    "\n",
    "First we make sure we have installed some basic things - json, numpy, sys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "252d46e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import sample\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f4dbaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ipywidgets as widgets\n",
    "except:\n",
    "    !pip install ipywidgets\n",
    "    import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0cd40",
   "metadata": {},
   "source": [
    "Second, we make sure we have installed spacy (which will split our input into words, assign a part of speech tag to each word and if necessary help us with named entity recognition), as well as python packages for:\n",
    "* syllabification - currently, we import SyllableTokenizer from nltk, and syllabifier which uses the CMU pronunciation dictionary\n",
    "* inflection - currently, we import lemminflect\n",
    "* rhyming - currently, we import SoundsLike\n",
    "and the nltk module for wordnet, to allow us to find words and calculate word distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448d075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from en-core-web-md==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.23.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: jinja2 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (22.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: setuptools in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (58.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (6.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/riley/Documents/poetry-project/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# see https://spacy.io/usage/spacy-101\n",
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    !pip install spacy==3.2.4\n",
    "    import spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204481d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package wordnet to /Users/riley/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/riley/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# see https://www.nltk.org/howto/wordnet.html\n",
    "# see https://www.nltk.org/api/nltk.tokenize.sonority_sequencing.html\n",
    "try:\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk.tokenize import SyllableTokenizer\n",
    "    !python -m nltk.downloader wordnet\n",
    "    !python -m nltk.downloader omw-1.4\n",
    "except:\n",
    "    !pip install nltk\n",
    "    !python -m nltk.downloader wordnet\n",
    "    !python -m nltk.downloader omw-1.4\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk.tokenize import SyllableTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a73cf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'syllabifier' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/anson-vandoren/syllabifier.git\n",
    "sys.path.insert(0, 'syllabifier')\n",
    "from syllabifier import cmuparser3\n",
    "from syllabifier.syllable3 import generate_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c313569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://github.com/bjascob/LemmInflect\n",
    "try:\n",
    "    from lemminflect import getInflection\n",
    "except:\n",
    "    !pip install lemminflect\n",
    "    from lemminflect import getInflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c26cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from SoundsLike.SoundsLike import Search\n",
    "except:\n",
    "    !pip install SoundsLike\n",
    "    from SoundsLike.SoundsLike import Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1044ed",
   "metadata": {},
   "source": [
    "## Let's Experiment With Different Syllabifiers\n",
    "\n",
    "Generally, the CMU dictionary-based one is more accurate. However, when it doesn't syllabify it doesn't syllabify *at all*. The other one always gives you something (albeit often incorrect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a950458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class syllabify:\n",
    "    def __init__(self, use_cmu = True):\n",
    "        if use_cmu:\n",
    "            self.syllabifier = cmuparser3.CMUDictionary()\n",
    "        else:\n",
    "            self.syllabifier = SyllableTokenizer()\n",
    "        self.use_cmu = use_cmu\n",
    "        \n",
    "    def syllabify(self, term):\n",
    "        if self.use_cmu:\n",
    "            phoneme_str = self.syllabifier.get_first(term)\n",
    "            if phoneme_str:\n",
    "                return [str(x) for x in generate_syllables(phoneme_str)]\n",
    "        else:\n",
    "            return self.syllabifier.tokenize(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1afdb1",
   "metadata": {},
   "source": [
    "# Process a Poem\n",
    "\n",
    "We take in a poem as a multi-line string. We split it on newlines so we can handle rhyming. We tokenize and part of speech tag. \n",
    "\n",
    "Then we choose 1 out of n \"content\" words (noun excluding proper nouns, verb, adjective, adverb) at random and replace it with another word having the same part of speech and number of syllables. If the chosen word is at the end of a line we try to ensure it rhymes.\n",
    "\n",
    "Future work:\n",
    "* ngrams / multiword expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f9e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "class poem_replacer:\n",
    "    def __init__(self, lexicon_file, use_cmu = True, use_pos = True, use_anagrams = False, use_rhyme = True, use_syllables = True, union_or_intersection = 'union', min_wn_distance = 0, max_wn_distance = -1):\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")\n",
    "        with open(lexicon_file) as f:\n",
    "            self.lexicon = json.load(f)\n",
    "        self.syllabifier = syllabify(use_cmu)\n",
    "        self.pos_mappings =  {'NOUN': wordnet.NOUN, 'VERB': wordnet.VERB, 'ADJ': wordnet.ADJ, 'ADV': wordnet.ADV}\n",
    "        self.use_cmu = use_cmu\n",
    "        self.use_pos = use_pos\n",
    "        self.use_anagrams = use_anagrams\n",
    "        self.use_rhyme = use_rhyme\n",
    "        self.use_syllables = use_syllables\n",
    "        self.union_or_intersection = union_or_intersection\n",
    "        self.min_wn_distance = min_wn_distance\n",
    "        self.max_wn_distance = max_wn_distance\n",
    "\n",
    "    def get_candidates_by_pos(self, term, pos):\n",
    "        if pos in self.lexicon['by_pos']:\n",
    "            possibles = self.lexicon['by_pos'][pos]\n",
    "            if possibles:\n",
    "                return [x for x in possibles if x != term]\n",
    "        return []\n",
    "    \n",
    "    def get_candidates_by_syllables(self, term):\n",
    "        syllables = self.syllabifier.syllabify(term)\n",
    "        if str(len(syllables)) in self.lexicon['by_syllables']:\n",
    "            possibles = self.lexicon['by_syllables'][str(len(syllables))]\n",
    "            if possibles:\n",
    "                return [x for x in possibles if x != term]\n",
    "        return []\n",
    "        \n",
    "    def get_candidates_by_anagrams(self, term):\n",
    "        if ''.join(sorted(term)) in self.lexicon['by_anagrams']:\n",
    "            possibles = self.lexicon['by_anagrams'][''.join(sorted(term))]\n",
    "            if possibles:\n",
    "                return [x for x in possibles if x != term]\n",
    "        return []\n",
    "    \n",
    "    def get_candidates_by_rhymes(self, term):\n",
    "        possibles = []\n",
    "        if term in self.lexicon['by_rhymes']:\n",
    "            possibles = self.lexicon['by_rhymes'][term]\n",
    "        syllables = self.syllabifier.syllabify(term)\n",
    "        if len(syllables) > 0 and syllables[-1] in self.lexicon['by_last_syllables']:\n",
    "            possibles = list(set(possibles).union(self.lexicon['by_last_syllables'][syllables[-1]]))\n",
    "        if possibles:\n",
    "            return [x for x in possibles if x != term]\n",
    "        return []\n",
    "        \n",
    "    def wn_distance(self, synset, possible, pos):\n",
    "        possible_synsets = wordnet.synsets(possible, self.pos_mappings[pos])\n",
    "        if len(possible_synsets) > 0:\n",
    "            dist = synset.wup_similarity(possible_synsets[0], simulate_root=False)\n",
    "            if dist is not None:\n",
    "                return dist\n",
    "        return float('inf')\n",
    "\n",
    "    def get_candidates(self, token, end_of_line = False):\n",
    "        pos_possibles = []\n",
    "        syllables_possibles = []\n",
    "        anagrams_possibles = []\n",
    "        rhyme_possibles = []\n",
    "        if self.use_pos:\n",
    "            pos_possibles = self.get_candidates_by_pos(token.text, token.pos_)\n",
    "        if self.use_syllables:\n",
    "            syllables_possibles = self.get_candidates_by_syllables(token.text)\n",
    "        if self.use_anagrams:\n",
    "            anagrams_possibles = self.get_candidates_by_anagrams(token.text)\n",
    "        if self.use_rhyme and end_of_line:\n",
    "            rhyme_possibles = self.get_candidates_by_rhymes(token.text)\n",
    "        if self.union_or_intersection == 'union':\n",
    "            possibles = list(set(pos_possibles).union(syllables_possibles, anagrams_possibles, rhyme_possibles))\n",
    "        else:\n",
    "            possibles_all = [x for x in [pos_possibles, syllables_possibles, anagrams_possibles, rhyme_possibles] if len(x) > 0]\n",
    "            if len(possibles_all) > 0:\n",
    "                possibles = possibles_all[0]\n",
    "                for possible in possibles_all[1:]:\n",
    "                    possibles = list(set(possibles).intersection(possible))\n",
    "            else:\n",
    "                possibles = []\n",
    "        # if min_wn_distance or max_wn_distance > -1, then we reduce candidates to those within min and max Wordnet distance of the token\n",
    "        # there are multiple Wordnet distance algorithms; we just use wup for now; see https://www.nltk.org/howto/wordnet.html\n",
    "        if self.min_wn_distance > 0:\n",
    "            t = wordnet.synsets(token.text, self.pos_mappings[token.pos_])\n",
    "            if len(t) > 0:\n",
    "                if self.max_wn_distance > -1:\n",
    "                    possibles = [possible for possible in possibles if len(wordnet.synsets(possible, self.pos_mappings[token.pos_])) > 0 and self.wn_distance(t[0], possible, token.pos_) in range(self.min_wn_distance, self.max_wn_distance)]\n",
    "                else:\n",
    "                    possibles = [possible for possible in possibles if len(wordnet.synsets(possible, self.pos_mappings[token.pos_])) > 0 and self.wn_distance(t[0], possible, token.pos_) > self.min_wn_distance]\n",
    "        return possibles                                 \n",
    "        \n",
    "    def get_tokens(self, text):\n",
    "        # make a spacy document from the input text\n",
    "        doc = self.nlp(text)\n",
    "        # get all the tokens\n",
    "        # see https://spacy.io/api/token\n",
    "        # see https://universaldependencies.org/u/pos/index.html\n",
    "        tokens = [token for token in doc]\n",
    "        content_tokens = [token for token in doc if token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV']]\n",
    "        return tokens, content_tokens               \n",
    "    \n",
    "    def process(self, text, percent_to_replace):                                 \n",
    "        tokens, content_tokens = self.get_tokens(text)\n",
    "        number_to_replace = int(len(content_tokens) * (percent_to_replace / 100))\n",
    "        # choose number_to_replace tokens to replace\n",
    "        to_replace = sample(content_tokens, number_to_replace)\n",
    "        # this contains the output words\n",
    "        words = []\n",
    "        # for each token\n",
    "        for i, token in enumerate(tokens):\n",
    "            # if the token is to be replaced\n",
    "            if token in to_replace:\n",
    "                # figure out if it is at the end of the poem or the end of a line; you can use this if you want to only match rhyme at end of line\n",
    "                end_of_line = (i == len(tokens)-1 or tokens[i+1].pos_ == 'SPACE')\n",
    "                # get possible replacement terms\n",
    "                possibles = self.get_candidates(token, end_of_line = end_of_line)\n",
    "                # choose one of them at random and replace!\n",
    "                if len(possibles) > 0:\n",
    "                    chosen = sample(possibles, 1)\n",
    "                    words.append(\"#REPLACE_TOKEN\" + chosen[0] + token.whitespace_)\n",
    "                else:\n",
    "                    words.append(token.text + token.whitespace_)\n",
    "            else:\n",
    "                words.append(token.text + token.whitespace_)\n",
    "        return number_to_replace, [x.text for x in to_replace], words     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a4443",
   "metadata": {},
   "source": [
    "### Let's Try It Out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c23bf9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wn_distances: part, understand, feel, style, useless You who in scattered rhymes listen to the sound \n",
      "Of those sighs with which I fed the heart\n",
      "During that first youthful mistake of mine\n",
      "When I was in #REPLACE_TOKENportions a different man than I am now\n",
      "I hope I can find forgiveness and pithy \n",
      "For the diverse #REPLACE_TOKENway in which I cry and reason,\n",
      "Between #REPLACE_TOKENuselessest hope and useless pain\n",
      "From those who #REPLACE_TOKENunderstanding love out of experience.\n",
      "Now I can see clearly how I have been a joke\n",
      "To all the people, for a long time, so much so that when I think about it,I often #REPLACE_TOKENfeeling ashamed of myself; \n",
      "Shame is the result of my ramblings,\n",
      "Along with regret, and the clear realization\n",
      "That what the world likes is but a brief dream\n"
     ]
    }
   ],
   "source": [
    "text = '''You who in scattered rhymes listen to the sound \n",
    "Of those sighs with which I fed the heart\n",
    "During that first youthful mistake of mine\n",
    "When I was in part a different man than I am now\n",
    "I hope I can find forgiveness and pithy \n",
    "For the diverse style in which I cry and reason,\n",
    "Between useless hope and useless pain\n",
    "From those who understand love out of experience.\n",
    "Now I can see clearly how I have been a joke\n",
    "To all the people, for a long time, so much so that when I think about it,I often feel ashamed of myself; \n",
    "Shame is the result of my ramblings,\n",
    "Along with regret, and the clear realization\n",
    "That what the world likes is but a brief dream'''\n",
    "\n",
    "# PR = poem_replacer('lexicon.json')\n",
    "# _, to_replace, words = PR.process(text, 10)\n",
    "# print('plain:', ', '.join(to_replace), ''.join(words))\n",
    "\n",
    "# PR = poem_replacer('lexicon.json', use_pos = True, use_anagrams = False, use_rhyme = False, use_syllables = False, union_or_intersection = 'intersection')\n",
    "# _, to_replace, words = PR.process(text, 10)\n",
    "# print('pos:', ', '.join(to_replace), ''.join(words))\n",
    "\n",
    "# PR = poem_replacer('lexicon.json', use_pos = False, use_anagrams = True, use_rhyme = False, use_syllables = False, union_or_intersection = 'intersection')\n",
    "# _, to_replace, words = PR.process(text, 10)\n",
    "# print('anagrams:', ', '.join(to_replace), ''.join(words))\n",
    "\n",
    "# PR = poem_replacer('lexicon.json', use_pos = False, use_anagrams = False, use_rhyme = True, use_syllables = False, union_or_intersection = 'intersection')\n",
    "# _, to_replace, words = PR.process(text, 10)\n",
    "# print('rhyme:', ', '.join(to_replace), ''.join(words))\n",
    "\n",
    "# PR = poem_replacer('lexicon.json', use_pos = False, use_anagrams = False, use_rhyme = False, use_syllables = True, union_or_intersection = 'intersection')\n",
    "# _, to_replace, words = PR.process(text, 10)\n",
    "# print('syllables:', ', '.join(to_replace), ''.join(words))\n",
    "\n",
    "# PR = poem_replacer('lexicon.json', use_pos = True, use_anagrams = True, use_rhyme = False, use_syllables = False, union_or_intersection = 'intersection')\n",
    "# _, to_replace, words = PR.process(text, 10)\n",
    "# print('pos, anagrams, intersection:', ', '.join(to_replace), ''.join(words))\n",
    "\n",
    "# PR = poem_replacer('lexicon.json', use_pos = True, use_anagrams = True, use_rhyme = False, use_syllables = False, union_or_intersection = 'union')\n",
    "# _, to_replace, words = PR.process(text, 10)\n",
    "# print('pos, anagrams, union:', ', '.join(to_replace), ''.join(words))\n",
    "\n",
    "PR = poem_replacer('lexicon.json', min_wn_distance=1, max_wn_distance=5)\n",
    "_, to_replace, words = PR.process(text, 10)\n",
    "print('wn_distances:', ', '.join(to_replace), ''.join(words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d6af10",
   "metadata": {},
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aa6dfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3debd012407458bbbef58e935fc3ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AppLayout(children=(VBox(children=(HTML(value='<H2>Poem Manipulator</H2>Enter your poem in the text area, sele…"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = widgets.HTML(\n",
    "    value=\"<H2>Poem Manipulator</H2>Enter your poem in the text area, select options beneath, choose number of substitutions, then click Manipulate!\")\n",
    "log = widgets.Output(layout={'border': '1px solid black', 'height': '40%', 'width': '97%'})\n",
    "with log:\n",
    "    print(\"Log notes will appear here\")\n",
    "header_box = widgets.VBox([html, log])\n",
    "\n",
    "input_poem = widgets.Textarea(value='Enter poem here', placeholder='Enter poem here', layout={'border': '1px solid black', 'height': '100%', 'width': '95%'})\n",
    "\n",
    "layout = widgets.Layout(width='auto', height='auto')\n",
    "substitution_slider = widgets.IntSlider(min=0, max=100, value=10)\n",
    "substitution_label = widgets.Label('Percent to replace: ', layout=widgets.Layout(width='40%'))\n",
    "substitution_box = widgets.HBox([substitution_label, substitution_slider])\n",
    "\n",
    "generate_button =  widgets.Button(description='Manipulate!', disabled=False)\n",
    "\n",
    "min_wn_slider = widgets.IntSlider(min=-1, max=10, value=0)\n",
    "min_wn_label = widgets.Label('Min WN distance: ', layout=widgets.Layout(width='40%'))\n",
    "min_wn_box = widgets.HBox([min_wn_label, min_wn_slider])\n",
    "\n",
    "max_wn_slider = widgets.IntSlider(min=-1, max=10, value=-1)\n",
    "max_wn_label = widgets.Label('Max WN distance: ', layout=widgets.Layout(width='40%'))\n",
    "max_wn_box = widgets.HBox([max_wn_label, max_wn_slider])\n",
    "\n",
    "pos = widgets.Checkbox(True, layout=widgets.Layout(justify_content=\"flex-start\"))\n",
    "pos_label = widgets.Label('Use Part of Speech: ', layout=widgets.Layout(width='40%'))\n",
    "pos_box = widgets.HBox([pos_label, pos])\n",
    "\n",
    "rhyme = widgets.Checkbox(True)\n",
    "rhyme_label = widgets.Label('Use Rhyme: ', layout=widgets.Layout(width='40%'))\n",
    "rhyme_box = widgets.HBox([rhyme_label, rhyme])\n",
    "\n",
    "anagrams = widgets.Checkbox(False)\n",
    "anagrams_label = widgets.Label('Use Anagrams: ', layout=widgets.Layout(width='40%'))\n",
    "anagrams_box = widgets.HBox([anagrams_label, anagrams])\n",
    "\n",
    "syllables = widgets.Checkbox(True)\n",
    "syllables_label = widgets.Label('Use Syllables: ', layout=widgets.Layout(width='40%'))\n",
    "syllables_box = widgets.HBox([syllables_label, syllables])\n",
    "\n",
    "uoi = widgets.RadioButtons(value='union', options=['union', 'intersection'])\n",
    "uoi_label = widgets.Label('Combine possibles by: ', layout=widgets.Layout(width='40%'))\n",
    "uoi_box = widgets.HBox([uoi_label, uoi])\n",
    "\n",
    "left_box = widgets.VBox([substitution_box, generate_button], layout=widgets.Layout(width='80%', height='auto'))\n",
    "right_box = widgets.VBox([min_wn_box, max_wn_box, pos_box, syllables_box, rhyme_box, anagrams_box, uoi_box], layout=widgets.Layout(width='80%', height='auto'))\n",
    "box = widgets.HBox([left_box, right_box])\n",
    "\n",
    "out = widgets.Output(layout={'border': '1px solid black', 'height': '100%', 'width': '95%'})\n",
    "with out:\n",
    "    print(\"Output poem will appear here\")\n",
    "    \n",
    "app = widgets.AppLayout(header=header_box, left_sidebar=input_poem, footer=box, center=None, right_sidebar=out)\n",
    "\n",
    "\n",
    "def process(b):\n",
    "    PR = poem_replacer('lexicon.json', use_cmu=True, use_pos=pos.value, use_anagrams=anagrams.value, use_syllables=syllables.value, use_rhyme=rhyme.value, union_or_intersection=uoi.value, min_wn_distance=min_wn_slider.value, max_wn_distance=max_wn_slider.value)\n",
    "    number_to_replace, to_replace, words = PR.process(input_poem.value, substitution_slider.value)\n",
    "    log.clear_output()\n",
    "    log.append_stdout(number_to_replace)\n",
    "    log.append_stdout('tokens to be replaced:' + ', '.join(to_replace))\n",
    "    out.clear_output()\n",
    "    out.append_stdout(''.join(words))\n",
    "\n",
    "generate_button.on_click(process)\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6219b84f",
   "metadata": {},
   "source": [
    "# Get a Lexicon\n",
    "\n",
    "**You should only have to do this section ONCE. It is slow.**\n",
    "\n",
    "We need a word list. For now, we use WordNet to get a word list. So from WordNet we extract all nouns, verbs, adjectives and adverbs. For each extracted term we collect all inflected forms.\n",
    "\n",
    "We then construct lexicons: one by part of speech, one by number of syllables, one by anagrams, and one by final syllable:\n",
    "* In order to handle replacement that respects syntax we need a lexicon by part of speech.\n",
    "* In order to handle replacement that respects rhythm we need a lexicon by number of syllables.\n",
    "* In order to handle replacement by anagrams we need a lexicon by anagrams.\n",
    "* In order to handle rhyming, we use the last syllable of each word; we also use the CMU pronunciation dictionary to find homophones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54f9e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordnet_lexicon:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    # Collect our lexicon by part of speech\n",
    "    def collect_terms(self, use_cmu = True):\n",
    "        # see https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "        types = {'NOUN': ['NN', 'NNS'], 'VERB': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'], 'ADJ': ['JJ', 'JJR', 'JJS'], 'ADV': ['RB', 'RBR', 'RBS']}\n",
    "        syllabifier = syllabify(use_cmu)\n",
    "        terms = {}\n",
    "        # Wordnet only contains these classes of word\n",
    "        for (tag, typ) in [(wordnet.NOUN, 'NOUN'), (wordnet.VERB, 'VERB'), (wordnet.ADJ, 'ADJ'), (wordnet.ADV, 'ADV')]:\n",
    "            print(\"Processing\", typ)\n",
    "            for synset in list(wordnet.all_synsets(tag)):\n",
    "                for lemma in synset.lemmas():\n",
    "                    # consider in future: keep multi-word expressions\n",
    "                    if '_' not in lemma.name():\n",
    "                        if lemma.name() not in terms:\n",
    "                            terms[lemma.name()] = {}\n",
    "                        for form in types[typ]:\n",
    "                            try:\n",
    "                                inflected = getInflection(lemma.name(), tag=form)\n",
    "                                if len(inflected) > 0 and inflected[0] not in terms[lemma.name()]:\n",
    "                                    try:\n",
    "                                        # could also use closeHomophones; may be slower\n",
    "                                        rhymes = Search.perfectHomophones(inflected[0])\n",
    "                                    except Exception as e:\n",
    "                                        rhymes = []\n",
    "                                    terms[lemma.name()][inflected[0]] = {'term': inflected[0], 'lemma': lemma.name(), 'pos': typ, 'syllables': syllabifier.syllabify(inflected[0]), 'letters': ''.join(sorted(inflected[0])), 'rhymes': rhymes}\n",
    "                            except Exception as e:\n",
    "                                print(\"error\", lemma.name(), form, e)\n",
    "        return terms #\n",
    "\n",
    "    # Collect terms; construct lexicons - one by part of speech, one by number of syllables, one by anagrams, and one by final syllable; dump to a file\n",
    "    def create_lexicon(self, lexicon_file, use_cmu = True):\n",
    "        terms = self.collect_terms(use_cmu = use_cmu)\n",
    "        lexicons = {'terms': terms, 'by_pos': {}, 'by_syllables': {}, 'by_last_syllables': {}, 'by_anagrams': {}, 'by_rhymes': {}}\n",
    "        for lemma in terms:\n",
    "            for entry in terms[lemma].values():\n",
    "                # add it to the part of speech list\n",
    "                if entry['pos'] not in lexicons['by_pos']:\n",
    "                    lexicons['by_pos'][entry['pos']] = []\n",
    "                lexicons['by_pos'][entry['pos']].append(entry['term'])\n",
    "                if entry['syllables'] is not None:\n",
    "                    # add it to the syllables list\n",
    "                    if len(entry['syllables']) not in lexicons['by_syllables']:\n",
    "                        lexicons['by_syllables'][len(entry['syllables'])] = []\n",
    "                    lexicons['by_syllables'][len(entry['syllables'])].append(entry['term'])\n",
    "                    # add it to the last-syllables list\n",
    "                    if entry['syllables'][-1] not in lexicons['by_last_syllables']:\n",
    "                        lexicons['by_last_syllables'][entry['syllables'][-1]] = []\n",
    "                    lexicons['by_last_syllables'][entry['syllables'][-1]].append(entry['term'])\n",
    "                # add it to the anagrams list\n",
    "                if entry['letters'] not in lexicons['by_anagrams']:\n",
    "                    lexicons['by_anagrams'][entry['letters']] = []\n",
    "                lexicons['by_anagrams'][entry['letters']].append(entry['term'])\n",
    "                # add it to the rhymes list\n",
    "                for rhyme in entry['rhymes']:\n",
    "                    if rhyme not in lexicons['by_rhymes']:\n",
    "                        lexicons['by_rhymes'][rhyme] = []\n",
    "                    lexicons['by_rhymes'][rhyme].append(entry['term'])                                    \n",
    "        with open(lexicon_file, 'w') as f:\n",
    "            json.dump(lexicons, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4835f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing NOUN\n",
      "error F NNS list index out of range\n",
      "error f NNS list index out of range\n",
      "error F NNS list index out of range\n",
      "error F NNS list index out of range\n",
      "error F NNS list index out of range\n",
      "error Th NNS list index out of range\n",
      "error Th NNS list index out of range\n",
      "Processing VERB\n",
      "Processing ADJ\n",
      "Processing ADV\n",
      "CPU times: user 1h 25min 45s, sys: 5min 7s, total: 1h 30min 53s\n",
      "Wall time: 9min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "WL = wordnet_lexicon()\n",
    "WL.create_lexicon('lexicon.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Aug  5 2022, 15:21:02) \n[Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb29b3c7d259ef4362384cf39fdfb5c94995bf960dfce8bdd17dff2317855276"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
